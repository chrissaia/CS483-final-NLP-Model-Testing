{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9418be7d",
   "metadata": {},
   "source": [
    "# DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "id": "62e1860f",
   "metadata": {},
   "source": [
    "# if needed, install required packages\n",
    "# %pip install -r ../requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81175b7e",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ea93ee3",
   "metadata": {},
   "source": [
    "# Load model + tokenizer\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90985eac",
   "metadata": {},
   "source": [
    "def classifier_fn(texts):\n",
    "    \"\"\"Takes a list of strings, returns probability array shape (batch_size, 2).\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probs = torch.softmax(logits, dim=1).numpy()\n",
    "    return probs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99c4cda7",
   "metadata": {},
   "source": [
    "class_names = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "explainer = LimeTextExplainer(class_names=class_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "080c6f76",
   "metadata": {},
   "source": [
    "### Basic LIME Explanation"
   ]
  },
  {
   "cell_type": "code",
   "id": "c220696c",
   "metadata": {},
   "source": [
    "sentence = \"I don't like this film.\"\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    sentence,\n",
    "    classifier_fn,\n",
    "    num_features=10\n",
    ")\n",
    "def lime_plot(exp):\n",
    "    html = exp.as_html()\n",
    "    with open(\"lime_explanation.html\", \"w\") as f:\n",
    "        f.write(html)\n",
    "\n",
    "    with open(\"lime_explanation.html\", \"r\") as f:\n",
    "        display_html = f.read()\n",
    "\n",
    "\n",
    "    from IPython.display import HTML\n",
    "    display(HTML(display_html))\n",
    "\n",
    "lime_plot(exp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35d3d255",
   "metadata": {},
   "source": [
    "### Negation Tests"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f50f652",
   "metadata": {},
   "source": [
    "negation_sentences = [\n",
    "    \"I like this movie.\",\n",
    "    \"I don't like this movie.\",\n",
    "    \"I do not like this movie.\",\n",
    "    \"This movie is not good.\",\n",
    "    \"This movie is not bad.\",\n",
    "    \"This movie is not bad at all.\"\n",
    "]\n",
    "\n",
    "negation_results = []\n",
    "\n",
    "for sent in negation_sentences:\n",
    "    exp = explainer.explain_instance(sent, classifier_fn, num_features=8)\n",
    "    important_words = exp.as_list()\n",
    "    pred = classifier_fn([sent])[0]\n",
    "\n",
    "    negation_results.append({\n",
    "        \"sentence\": sent,\n",
    "        \"prediction_NEG\": float(pred[0]),\n",
    "        \"prediction_POS\": float(pred[1]),\n",
    "        \"important_words\": important_words\n",
    "    })\n",
    "\n",
    "    lime_plot(exp)\n",
    "\n",
    "pd.DataFrame(negation_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1938cd39",
   "metadata": {},
   "source": [
    "### Emotional Word Weighting"
   ]
  },
  {
   "cell_type": "code",
   "id": "89e1f00e",
   "metadata": {},
   "source": [
    "emotional_tests = [\n",
    "    \"The plot was boring but the cinematography was stunning.\",\n",
    "    \"The movie was absolutely amazing but the acting was awful.\",\n",
    "    \"The movie was decent but not great.\",\n",
    "    \"The characters were fantastic and the music was terrible.\",\n",
    "]\n",
    "\n",
    "emotional_results = []\n",
    "\n",
    "for sent in emotional_tests:\n",
    "    exp = explainer.explain_instance(sent, classifier_fn, num_features=8)\n",
    "    important = exp.as_list()\n",
    "    probs = classifier_fn([sent])[0]\n",
    "\n",
    "    emotional_results.append({\n",
    "        \"sentence\": sent,\n",
    "        \"positive_prob\": float(probs[1]),\n",
    "        \"negative_prob\": float(probs[0]),\n",
    "        \"important_words\": important\n",
    "    })\n",
    "\n",
    "    lime_plot(exp)\n",
    "\n",
    "pd.DataFrame(emotional_results)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8bd5f4ca",
   "metadata": {},
   "source": [
    "### Sarcasm Tests"
   ]
  },
  {
   "cell_type": "code",
   "id": "295e4ccf",
   "metadata": {},
   "source": [
    "sarcasm_tests = [\n",
    "    \"Great. Just what I needed today.\",\n",
    "    \"Fantastic job ruining everything.\",\n",
    "    \"I totally loved waiting 45 minutes in line.\",\n",
    "]\n",
    "\n",
    "sarcasm_results = []\n",
    "\n",
    "for sent in sarcasm_tests:\n",
    "    exp = explainer.explain_instance(sent, classifier_fn, num_features=8)\n",
    "    important = exp.as_list()\n",
    "    probs = classifier_fn([sent])[0]\n",
    "\n",
    "    sarcasm_results.append({\n",
    "        \"sentence\": sent,\n",
    "        \"positive_prob\": float(probs[1]),\n",
    "        \"negative_prob\": float(probs[0]),\n",
    "        \"important_words\": important\n",
    "    })\n",
    "\n",
    "    lime_plot(exp)\n",
    "\n",
    "pd.DataFrame(sarcasm_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c8f888c",
   "metadata": {},
   "source": [
    "### Fairness Tests"
   ]
  },
  {
   "cell_type": "code",
   "id": "2af19652",
   "metadata": {},
   "source": [
    "fairness_tests = [\n",
    "    \"He is a doctor.\",\n",
    "    \"She is a doctor.\",\n",
    "    \"He is a nurse.\",\n",
    "    \"She is a nurse.\",\n",
    "    \"He is a leader.\",\n",
    "    \"She is a leader.\",\n",
    "]\n",
    "\n",
    "fairness_results = []\n",
    "\n",
    "for sent in fairness_tests:\n",
    "    exp = explainer.explain_instance(sent, classifier_fn, num_features=5)\n",
    "    important = exp.as_list()\n",
    "    probs = classifier_fn([sent])[0]\n",
    "\n",
    "    fairness_results.append({\n",
    "        \"sentence\": sent,\n",
    "        \"positive_prob\": float(probs[1]),\n",
    "        \"negative_prob\": float(probs[0]),\n",
    "        \"important_words\": important\n",
    "    })\n",
    "\n",
    "    lime_plot(exp)\n",
    "\n",
    "\n",
    "pd.DataFrame(fairness_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b19ac2d8",
   "metadata": {},
   "source": [
    "### Adversarial / Misspelling Robustness"
   ]
  },
  {
   "cell_type": "code",
   "id": "540b0cc4",
   "metadata": {},
   "source": [
    "adversarial_tests = [\n",
    "    \"This movie was good.\",\n",
    "    \"This movie was gooood.\",\n",
    "    \"This movie was gud.\",\n",
    "    \"This movie was goood!\",\n",
    "    \"This movie was good??\",\n",
    "    \"This movie was bad.\",\n",
    "]\n",
    "\n",
    "adv_results = []\n",
    "\n",
    "for sent in adversarial_tests:\n",
    "    exp = explainer.explain_instance(sent, classifier_fn, num_features=6)\n",
    "    probs = classifier_fn([sent])[0]\n",
    "    important = exp.as_list()\n",
    "\n",
    "    adv_results.append({\n",
    "        \"sentence\": sent,\n",
    "        \"positive_prob\": float(probs[1]),\n",
    "        \"negative_prob\": float(probs[0]),\n",
    "        \"important_words\": important\n",
    "    })\n",
    "\n",
    "    lime_plot(exp)\n",
    "\n",
    "pd.DataFrame(adv_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27f46d61",
   "metadata": {},
   "source": [
    "### Mixed Sentiment / Multi-Clause"
   ]
  },
  {
   "cell_type": "code",
   "id": "f34ab224",
   "metadata": {},
   "source": [
    "mixed_tests = [\n",
    "    \"The acting was amazing but the plot was boring.\",\n",
    "    \"The visuals were incredible but the writing was weak.\",\n",
    "    \"The first half was great, the second half was terrible.\"\n",
    "]\n",
    "\n",
    "mixed_results = []\n",
    "\n",
    "for sent in mixed_tests:\n",
    "    exp = explainer.explain_instance(sent, classifier_fn, num_features=8)\n",
    "    important = exp.as_list()\n",
    "    probs = classifier_fn([sent])[0]\n",
    "\n",
    "    mixed_results.append({\n",
    "        \"sentence\": sent,\n",
    "        \"positive_prob\": float(probs[1]),\n",
    "        \"negative_prob\": float(probs[0]),\n",
    "        \"important_words\": important\n",
    "    })\n",
    "\n",
    "    lime_plot(exp)   \n",
    "\n",
    "pd.DataFrame(mixed_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "962edba7",
   "metadata": {},
   "source": [
    "### How LIME Perturbation Works w/ Lime Maps"
   ]
  },
  {
   "cell_type": "code",
   "id": "ab4f5f3a",
   "metadata": {},
   "source": [
    "sentence = \"I don't like this film.\"\n",
    "\n",
    "exp = explainer.explain_instance(sentence, classifier_fn, num_features=10)\n",
    "\n",
    "# LIME internal token importance maps\n",
    "lime_map = exp.as_map()\n",
    "lime_map"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "780e7501",
   "metadata": {},
   "source": [
    "### Count most influential words across all categories:"
   ]
  },
  {
   "cell_type": "code",
   "id": "92738653",
   "metadata": {},
   "source": [
    "all_sentences = negation_sentences + emotional_tests + sarcasm_tests + fairness_tests + mixed_tests\n",
    "\n",
    "word_importance_records = []\n",
    "\n",
    "for sent in all_sentences:\n",
    "    exp = explainer.explain_instance(sent, classifier_fn, num_features=10)\n",
    "    for word, score in exp.as_list():\n",
    "        word_importance_records.append((word, score))\n",
    "\n",
    "df_importance = pd.DataFrame(word_importance_records, columns=[\"word\", \"importance\"])\n",
    "df_importance.groupby(\"word\")[\"importance\"].mean().sort_values(ascending=True).head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e578770",
   "metadata": {},
   "source": [
    "df_importance.groupby(\"word\")[\"importance\"].mean().sort_values(ascending=False).head(10)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
